{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we use the output from the topic model and our immersion journal/manual to subset a training set of tweets about the Australian Bushfires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing relevant packages\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm #to create a progress bar\n",
    "\n",
    "#Load custom function for preprocessing the text\n",
    "from tokenizer import preprocess, preprocess_lemma, preprocess_stem\n",
    "\n",
    "#Machine learning packages\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Packages to create DFM\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#Packages for cross-validation and parameter tuning\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Packages for getting model performance metrics\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Packages for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "\n",
    "#Gary King et. al key-words\n",
    "from keyword_algorithm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData:\n",
    "    \"\"\"\n",
    "    This class prepares the data for the topic classification model\n",
    "    \"\"\"\n",
    "    \n",
    "    def naive_classification(self, df, keywords):\n",
    "        \n",
    "        #Subset on relevant period\n",
    "        df = df.loc[(df[\"created_at\"] >= \"2019-06-01\") & (df[\"created_at\"] <= \"2020-06-01\")]\n",
    "        \n",
    "        #Join keywords to regex\n",
    "        keywords = '|'.join(keywords)\n",
    "        df[\"bushfire\"] = df[\"stemmed_text\"].str.contains(keywords, regex = True).astype(int)\n",
    "        \n",
    "        #Remove the same keywords for text to avoid overfitting \n",
    "        df[\"stemmed_text\"] = df[\"stemmed_text\"].str.replace(keywords, \"\", regex = True)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "        \n",
    "    def prepare_text(self, df):\n",
    "        \n",
    "        df[\"clean_text\"] = df[\"full_text\"].apply(lambda x: preprocess(x))\n",
    "        df[\"stemmed_text\"] = df[\"clean_text\"].apply(lambda x: preprocess_stem(x))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def compile_df(self, df, keywords):\n",
    "        \"\"\"\n",
    "        Performs classification and subsetting of tweets. \n",
    "        Returns cleaned \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        df = self.prepare_text(df)\n",
    "        df = self.naive_classification(df, keywords)\n",
    "        \n",
    "        \n",
    "        return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Gary Kings semi-automated keyword retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/final_tweet_df\", index_col=0)\n",
    "#Subset period June 2019 – May 2020\n",
    "subset = data.loc[(data[\"created_at\"] >= \"2019-06-01\") & (data[\"created_at\"] <= \"2020-06-01\")]\n",
    "subset[\"id\"] = subset.index\n",
    "\n",
    "\n",
    "#Create a reference set of tweets\n",
    "reference_set = subset.loc[subset[\"full_text\"].str.contains(\"bushfire\")]\n",
    "#Create a search set of tweets\n",
    "search_set = subset[~subset.index.isin(reference_set.index)]\n",
    "\n",
    "reference_set.to_csv(\"data/reference_set.csv\")\n",
    "search_set.to_csv(\"data/search_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword object initialized.\n",
      "Loaded reference set of size 1208 in 0.03 seconds.\n",
      "Loaded search set of size 45265 in 0.36 seconds.\n",
      "Time to process corpus: 14.19 seconds\n",
      "\n",
      "4265 reference set keywords found.\n",
      "\n",
      "Document Term Matrix: 46473 by 61309 with 655324 nonzero elements\n",
      "\n",
      "Time to get document-term matrix: 0.51 seconds\n",
      "\n",
      "Ref training size: 399; Search training size: 14937; Training size: 15336; Test size: 45265\n",
      "\n",
      "Time for Naive Bayes: 0.01 seconds\n",
      "Time for Logit: 1.08 seconds\n",
      "169 documents in target set\n",
      "45096 documents in non-target set\n",
      "505 target set keywords found\n",
      "4964 non-target set keywords found\n"
     ]
    }
   ],
   "source": [
    "#Import all the methods from King et. al.\n",
    "#Note that some methods have been updated because of depreceated Pandas version\n",
    "bushfire = Keywords()\n",
    "bushfire.ReferenceSet(data='data/reference_set.csv', text_colname='full_text', id_colname='id')\n",
    "bushfire.SearchSet(data='data/search_set.csv', text_colname='full_text', id_colname='id')\n",
    "bushfire.ProcessData(remove_wordlist=[], keep_twitter_symbols=False)\n",
    "bushfire.ReferenceKeywords()\n",
    "bushfire.ClassifyDocs(algorithms=['nbayes', 'logit'])# 'tree', 'gboost'])\n",
    "bushfire.FindTargetSet()\n",
    "bushfire.FindKeywords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Reference                  Target                        Non-target\n",
      "   ----------                 ----------                    ----------\n",
      "1. bushfir                    bushfir                       auspol\n",
      "2. support                    recoveri                      australian\n",
      "3. affect                     cfsalert                      one\n",
      "4. amp                        area                          great\n",
      "5. communiti                  assist                        job\n",
      "6. australia                  vicemerg                      peopl\n",
      "7. today                      affect                        year\n",
      "8. govern                     reservist                     liber\n",
      "9. help                       fire                          mani\n",
      "10. need                      brigad                        labor\n",
      "11. crisi                     vicfir                        teen\n",
      "12. recoveri                  chip                          markbaileymp\n",
      "13. australian                intens                        crow\n",
      "14. area                      con                           auxiliari\n",
      "15. servic                    hazard                        thunberg\n",
      "16. auspol                    monitor                       maris\n",
      "17. get                       enact                         auto\n",
      "18. fire                      safir                         racket\n",
      "19. peopl                     smoke                         crush\n",
      "20. nsw                       longer                        avenu\n",
      "21. thank                     habitat                       horrend\n",
      "22. busi                      alert                         mateship\n",
      "23. morrison                  immedi                        racgp\n",
      "24. work                      compulsori                    mattcowgil\n",
      "25. emerg                     tumut                         curb\n",
      "26. call                      nswfire                       matthewson\n",
      "27. disast                    wildlif                       hontonyabbott\n",
      "28. continu                   logist                        quentindempst\n",
      "29. chang                     advic                         mayo\n",
      "30. impact                    bbq                           excess\n",
      "31. dure                      initi                         celest\n",
      "32. pleas                     addit                         virginia\n",
      "33. climat                    season                        virtu\n",
      "34. assist                    survivor                      caution\n",
      "35. devast                    disast                        sheep\n",
      "36. respons                   personnel                     realist\n",
      "37. new                       wale                          https://t.co/sytlfrskhu\n",
      "38. local                     bega                          gem\n",
      "39. centr                     adf                           realbobkatt\n",
      "40. small                     canungra                      barrack\n",
      "41. nation                    johnbarilaromp                fain\n",
      "42. one                       tyer                          creator\n",
      "43. provid                    pearson                       gen\n",
      "44. day                       doc                           luca\n",
      "45. govt                      revis                         https://t.co/fwcjgb5l5q\n",
      "46. week                      administ                      lump\n",
      "47. visit                     armidal                       mccallum\n",
      "48. health                    ford                          lyon\n",
      "49. back                      cessnock                      exposur\n",
      "50. albomp                    sup…                          comp\n",
      "51. season                    yamba                         madrid\n",
      "52. payment                   rori                          magazin\n",
      "53. great                     fyi                           mainland\n",
      "54. region                    pile                          crn\n",
      "55. time                      deploy                        awst\n",
      "56. fund                      gladston                      vile\n",
      "57. see                       album                         vinni\n",
      "58. say                       confidentgirl                 marap\n",
      "59. volunt                    kempsey                       cushion\n",
      "60. mani                      permit                        waca\n",
      "61. hit                       fi…                           exceed\n",
      "62. lovegippsland             nacchochair                   dcau\n",
      "63. via                       com…                          michaelkoziol\n",
      "64. famili                    duncan                        hildebrand\n",
      "65. live                      shoalhaven                    entrant\n",
      "66. relief                    mineralscouncil               warwick\n",
      "67. mobil                     inverel                       davelennonabc\n",
      "68. year                      jingel                        arnottsbikki\n",
      "69. updat                     messag                        goldstein\n",
      "70. recent                    elliot                        davidsharaz\n",
      "71. firefight                 lazi                          dawson\n",
      "72. victim                    iview                         archiv\n",
      "73. avail                     janeenorman                   enshrin\n",
      "74. media                     oper                          gorman\n",
      "75. scott                     ndia                          chancellor\n",
      "76. lost                      brutal                        mi…\n",
      "77. make                      trigger                       starmer\n",
      "78. safe                      coffsharbour                  mobilis\n",
      "79. way                       concert                       deansmithwa\n",
      "80. fight                     kwinana                       applaud\n",
      "81. across                    bathurst                      mogo\n",
      "82. come                      richarddinatal                heather\n",
      "83. face                      ulladulla                     cheat\n",
      "84. effort                    queanbeyan                    hearten\n",
      "85. donat                     aedt                          toughest\n",
      "86. import                    mid                           morgan\n",
      "87. home                      clarifi                       propaganda\n",
      "88. summer                    kel                           metronet\n",
      "89. worst                     regim                         lockyer\n",
      "90. take                      dunn                          shameless\n",
      "91. month                     misl                          auschwitz\n",
      "92. labor                     gordon                        qta\n",
      "93. keep                      ssfcrabbitoh                  ausambathen\n",
      "94. daili                     underpin                      mcmillan\n",
      "95. mental                    tender                        ftas\n",
      "96. immedi                    aspi                          mcnamara\n",
      "97. discuss                   thelandnew                    tracker\n",
      "98. feder                     threat                        med\n",
      "99. youradf                   catastroph                    sausag\n",
      "100. well                     stay                          psa\n"
     ]
    }
   ],
   "source": [
    "bushfire.PrintKeywords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the key-words from Kings model to subset tweets. Next we use this subset to predict the party of the tweet and analyze the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract most predictive keywords from model\n",
    "keywords = list(set(bushfire.target_keywords[:1] + bushfire.reference_keywords[:1]))\n",
    "\n",
    "#Initialize the \n",
    "dataprep = PrepareData()\n",
    "\n",
    "data = pd.read_csv(\"data/final_tweet_df\", index_col=0)\n",
    "df = dataprep.compile_df(data, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate the data on Party affiliation and day\n",
    "df_agg = df.groupby(['created_at','party']).agg({\"stemmed_text\":\" \".join}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Liberal Party of Australia',\n",
       "       'Liberal National Party of Queensland', 'Australian Labor Party',\n",
       "       'The Nationals', 'Centre Alliance', 'Nick Xenophon Team',\n",
       "       'Australian Greens', 'Independent', \"Katter's Australian Party\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"party\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = df_agg.loc[df_agg[\"party\"].isin([\"Australian Greens\",\"Australian Labor Party\", \"Liberal Party of Australia\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 1 epochs took 0 seconds\n",
      "convergence after 3 epochs took 0 seconds\n",
      "convergence after 3 epochs took 0 seconds\n",
      "convergence after 3 epochs took 0 seconds\n",
      "convergence after 3 epochs took 0 seconds\n",
      "convergence after 20 epochs took 0 seconds\n",
      "convergence after 18 epochs took 0 seconds\n",
      "convergence after 21 epochs took 0 seconds\n",
      "convergence after 23 epochs took 0 seconds\n",
      "convergence after 21 epochs took 0 seconds\n",
      "convergence after 254 epochs took 8 seconds\n",
      "convergence after 261 epochs took 8 seconds\n",
      "convergence after 323 epochs took 9 seconds\n",
      "convergence after 337 epochs took 10 seconds\n",
      "convergence after 387 epochs took 11 seconds\n",
      "convergence after 1220 epochs took 36 seconds\n",
      "convergence after 1254 epochs took 38 seconds\n",
      "convergence after 1312 epochs took 40 seconds\n",
      "convergence after 1604 epochs took 46 seconds\n",
      "convergence after 2260 epochs took 63 seconds\n",
      "convergence after 1516 epochs took 58 seconds\n",
      "convergence after 1643 epochs took 61 seconds\n",
      "convergence after 1452 epochs took 53 seconds\n",
      "convergence after 1695 epochs took 65 seconds\n",
      "convergence after 1898 epochs took 62 seconds\n",
      "convergence after 598 epochs took 49 seconds\n",
      "convergence after 816 epochs took 63 seconds\n",
      "convergence after 916 epochs took 71 seconds\n",
      "convergence after 1014 epochs took 75 seconds\n",
      "convergence after 122 epochs took 22 seconds\n",
      "convergence after 17 epochs took 3 seconds\n",
      "convergence after 3 epochs took 1 seconds\n",
      "convergence after 2 epochs took 1 seconds\n",
      "convergence after 825 epochs took 58 seconds\n",
      "convergence after 180 epochs took 25 seconds\n",
      "convergence after 185 epochs took 23 seconds\n",
      "convergence after 21 epochs took 3 seconds\n",
      "convergence after 3 epochs took 1 seconds\n",
      "convergence after 2 epochs took 0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  3.4min remaining:  5.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 22 epochs took 4 seconds\n",
      "convergence after 3 epochs took 0 seconds\n",
      "convergence after 2 epochs took 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:  3.4min remaining:  2.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 265 epochs took 32 seconds\n",
      "convergence after 168 epochs took 17 seconds\n",
      "convergence after 21 epochs took 5 seconds\n",
      "convergence after 41 epochs took 7 seconds\n",
      "convergence after 3 epochs took 1 seconds\n",
      "convergence after 2 epochs took 0 seconds\n",
      "convergence after 4 epochs took 1 seconds\n",
      "convergence after 2 epochs took 1 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  3.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 317 epochs took 46 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "X, y = df_agg[\"stemmed_text\"], df_agg[\"party\"]\n",
    "\n",
    "vect = CountVectorizer(max_features=10000)\n",
    "X = vect.fit_transform(X)\n",
    "\n",
    "clf = LogisticRegressionCV(cv=5, random_state=42, penalty=\"l1\", solver=\"saga\", \n",
    "                           max_iter = 10000, n_jobs = -1, verbose=10, \n",
    "                           class_weight = \"balanced\").fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginal_effect(model):\n",
    "    \"\"\"\n",
    "    This function takes the multinomial logistic model and calculates the marginal effect\n",
    "    of one added occurance of word k in post i. \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Get predictions and coefficients\n",
    "    preds = model.predict_proba(X) # Shape discplines * N posts i.e. (6* approx. 246000)\n",
    "    coefs = model.coef_ # Shape discplines * N word tokens i.e. (6*10000))\n",
    "    \n",
    "    # Define the denominator to calculate predicted \n",
    "    # probability of post i, for each field\n",
    "   \n",
    "    denominator = sum([np.exp(preds[:,label]) for label in range(len(model.classes_))])\n",
    "    \n",
    "    # Calculate probability of each post i being about each field\n",
    "    probas = [(np.exp(preds[:,label]) / denominator) for label in range(len(model.classes_))]\n",
    "    \n",
    "    # initiate \n",
    "    MEs = []\n",
    "    \n",
    "    for index, label_proba in tqdm(enumerate(probas)):\n",
    "        \n",
    "        temp = []\n",
    "        for beta_k in coefs[index]:\n",
    "            p_sum = (label_proba* (1-label_proba)).sum()\n",
    "            me = round((1 / len(probas[0])) * beta_k * p_sum, 3)\n",
    "            temp.append(me)\n",
    "        \n",
    "        MEs.append(temp)\n",
    "    \n",
    "    X_tokens = vect.get_feature_names()\n",
    "    df_data = {label:MEs[i] for i, label in enumerate(clf.classes_)}\n",
    "    df_data[\"word\"] = X_tokens #Add word tokens\n",
    "    \n",
    "    me_df = pd.DataFrame(df_data)\n",
    "    \n",
    "    \n",
    "    return me_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00,  6.62it/s]\n"
     ]
    }
   ],
   "source": [
    "me_df = marginal_effect(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australian Greens</th>\n",
       "      <th>Australian Labor Party</th>\n",
       "      <th>Liberal Party of Australia</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3867</th>\n",
       "      <td>0.211</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>green</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713</th>\n",
       "      <td>0.143</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>coal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1669</th>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>climat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3811</th>\n",
       "      <td>0.111</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>gov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6743</th>\n",
       "      <td>0.072</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>0.058</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>protest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7026</th>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6577</th>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>peopl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7529</th>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>crisi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5388</th>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>make</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5151</th>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>lib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6786</th>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>pollut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9658</th>\n",
       "      <td>0.036</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3626</th>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>ga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3356</th>\n",
       "      <td>0.032</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7895</th>\n",
       "      <td>0.029</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>senat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>aust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9421</th>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4736</th>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>job</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Australian Greens  Australian Labor Party  Liberal Party of Australia  \\\n",
       "3867              0.211                  -0.032                      -0.041   \n",
       "1713              0.143                  -0.008                      -0.042   \n",
       "1669              0.126                  -0.002                      -0.162   \n",
       "3811              0.111                  -0.013                      -0.015   \n",
       "6743              0.072                  -0.011                      -0.028   \n",
       "6999              0.058                  -0.023                      -0.010   \n",
       "7026              0.053                  -0.010                      -0.025   \n",
       "6577              0.051                  -0.001                      -0.065   \n",
       "7529              0.046                  -0.020                      -0.009   \n",
       "2163              0.041                  -0.000                      -0.042   \n",
       "5388              0.041                  -0.003                      -0.021   \n",
       "5151              0.038                  -0.007                      -0.006   \n",
       "6786              0.036                  -0.010                      -0.007   \n",
       "9658              0.036                  -0.029                      -0.002   \n",
       "3626              0.034                  -0.012                      -0.006   \n",
       "3356              0.032                  -0.015                      -0.008   \n",
       "7895              0.029                  -0.009                      -0.006   \n",
       "613               0.028                  -0.017                      -0.005   \n",
       "9421              0.028                  -0.001                      -0.024   \n",
       "4736              0.028                  -0.017                      -0.003   \n",
       "\n",
       "         word  \n",
       "3867    green  \n",
       "1713     coal  \n",
       "1669   climat  \n",
       "3811      gov  \n",
       "6743       pm  \n",
       "6999  protest  \n",
       "7026   public  \n",
       "6577    peopl  \n",
       "7529    right  \n",
       "2163    crisi  \n",
       "5388     make  \n",
       "5151      lib  \n",
       "6786   pollut  \n",
       "9658      way  \n",
       "3626       ga  \n",
       "3356     fire  \n",
       "7895    senat  \n",
       "613      aust  \n",
       "9421       us  \n",
       "4736      job  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me_df.sort_values(\"Australian Greens\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
